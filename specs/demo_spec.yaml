# this is a demo file for experiment spec
# please refer to the code for more parameter usage

# meta_data is passed to 'run_experiment.py'
meta_data:
  script_path: run_scripts/coil_script.py # the script to run
  exp_name: demo_exp # name of the experiment, used for generating names of log files
  description: this is a demo file # description, for human reading
  # srun settings
  num_workers: 2
  num_gpu_per_worker: 1
  num_cpu_per_worker: 32
  mem_per_worker: 4gb
  partitions: cpu
  node_exclusions: gpu048,gpu024,gpu025,gpu012,gpu027
  extra_info: extra_infos # used for generating names of log files, could leave blank

# -----------------------------------------------------------------------------
# each combination generates an experiment process
# could be used for param searching or multi-seed running
variables:
  offline_params:
    num_bc_update_loops_per_train_call: [ 100 ]
    filter_tau: [ 0.85, 0.9, 0.95 ]
  bc_traj_limit: [ 2 ]
  traj_sel_crt: [ alpha2 ]
  seed: [ 0, 1, 2 ]

# -----------------------------------------------------------------------------
# params already in 'variables' cannot duplicate in 'constants'
constants:
  test: false
  dataset_name: 'hopper_mr' # the list of names of dataset is in 'demos_listing.yaml'
  scale_env_with_demo_stats: false
  only_bc: true

  policy_net_size: 256
  policy_num_hidden_layers: 2

  sub_buf_size: 1000000
  use_epsilon_decay: false
  epsilon_min_or_init: 0.5
  epsilon_decay: 1.5
  bc_sampling_with_rep: true
  rl_traj_limit: 4
  filter_percent: 1.00
  gaussian_std: 1.0

  offline_params:
    mode: 'reward'

    num_epochs: 1000
    num_steps_per_epoch: 10000
    num_steps_between_train_calls: 2500
    max_path_length: 1000
    min_steps_before_training: 0

    eval_deterministic: true # use deterministic policy in online evaluation
    num_steps_per_eval: 50000

    no_terminal: false
    wrap_absorbing: false

    num_policy_update_loops_per_train_call: 20
    num_policy_updates_per_loop_iter: 1

    use_grad_pen: false
    policy_optim_batch_size: 256
    policy_optim_batch_size_from_expert: 0

    save_best: true
    freq_saving: 20
    save_replay_buffer: false
    save_environment: false
    save_algorithm: false

    filter_type: running_mean
    shrink_buffer: false

  coil_params:
    beta_1: 0.25
    policy_lr: 0.0003
    policy_mean_reg_weight: 0.001
    policy_std_reg_weight: 0.001
    use_l2: false

  env_specs:
    env_name: 'hopper' # environment name
    env_kwargs: { }
    eval_env_seed: 78236
    training_env_seed: 24495